{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P9ZirIEiQ-1K"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive')\n",
        "\n",
        "%cd /gdrive/MyDrive/Colab Notebooks"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "#依照所需修改下面參數:\n",
        "# 1.資料路徑\n",
        "DATASET_PATH  = 'Two-Class-ori-1/Image_result'\n",
        "# 2.影像大小\n",
        "IMAGE_SIZE = (227,227) #(224,224) (227,227) (299,299)\n",
        "\n",
        "# 3.影像類別數\n",
        "NUM_CLASSES = 2 # 5\n",
        "\n",
        "# 4.若 GPU 記憶體不足，可調降 batch size 或凍結更多層網路\n",
        "BATCH_SIZE = 16\n",
        "\n",
        "# 5.Epoch數\n",
        "NUM_EPOCHS = 84\n",
        "\n",
        "# 6.模型輸出儲存的檔案\n",
        "WEIGHTS_FINAL = 'Two-Class-ori-1/Image_result/test5.h5'\n",
        "\n",
        "#讀取路徑下圖片\n",
        "train_datagen = ImageDataGenerator(rescale=1./255)\n",
        "train_batches = train_datagen.flow_from_directory('Two-Class-ori-1/Image_result/',\n",
        "                                                  target_size=IMAGE_SIZE,\n",
        "                                                  interpolation='bicubic',\n",
        "                                                  class_mode='categorical',\n",
        "                                                  shuffle=True,\n",
        "                                                  batch_size=BATCH_SIZE) #color_mode='grayscale',\n",
        "\n",
        "valid_datagen = ImageDataGenerator(rescale=1./255)\n",
        "valid_batches = valid_datagen.flow_from_directory('Two-Class-ori-1/valid/',\n",
        "                                                  target_size=IMAGE_SIZE,\n",
        "                                                  interpolation='bicubic',\n",
        "                                                  class_mode='categorical',\n",
        "                                                  shuffle=False,\n",
        "                                                  batch_size=BATCH_SIZE)\n",
        "#print(DATASET_PATH + 'train\\\\')\n",
        "print(train_batches.image_shape)\n",
        "print(valid_batches.image_shape)"
      ],
      "metadata": {
        "id": "jHHR1mPVQ_rh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#使用Callbacks，使訓練模型Early Stop\n",
        "from keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "\n",
        "#為了防止overfitting，在10個epoch內 val_loss數值都沒有下降時會停止模型的訓練\n",
        "earlystop = EarlyStopping(patience=10) #callback要設定\n",
        "\n",
        "#Learning Rate Reduction 當訓練已無改善時，降低學習率\n",
        "\n",
        "#googlenet模型: monitor =val_auxilliary_output_2_accuracy\n",
        "#其他分類模型 : monitor =val_accuracy\n",
        "learning_rate_reduction = ReduceLROnPlateau(monitor = 'val_auxilliary_output_2_accuracy',   \n",
        "                                            patience = 2,#代表模型連續訓練2個epoch後，監控的數值表現沒有更好，就會停止訓練。\n",
        "                                            verbose = 1, #用於選擇模型訓練資訊的顯示方式。\n",
        "                                            factor = 0.5,#缩放學習率的常數。訓練模型中，若符合patience條件\n",
        "                                            #(如：連續訓練5個epoch後，val_loss未降低)，則將原本的學習率值乘以factor，做為新的學習率。\n",
        "                                            min_lr = 0.00001#每當符合patience條件，學習率會變動(縮小)，min_lr代表學習率的下限。\n",
        "                                            )\n",
        "\n",
        "#模型效果變好就保存，最後儲存最好的模型\n",
        "filepath=\"'Two-Class-ori/models/best.h5\"\n",
        "checkpoint = ModelCheckpoint(filepath, monitor='val_auxilliary_output_2_accuracy', verbose=1, save_best_only=True,mode='max')\n",
        "\n",
        "callbacks = [learning_rate_reduction,checkpoint]"
      ],
      "metadata": {
        "id": "hYXgvDobRB18"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#建立模型\n",
        "#Alexnet (image size 227x227)\n",
        "\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.layers import Flatten, Dense, Dropout,Conv2D, MaxPooling2D\n",
        "\n",
        "# 建立模型序列\n",
        "model = Sequential()\n",
        "#第一層卷積網路，使用96個卷積核，大小為11x11步長為4， 要求輸入的圖片為227x227， 3個通道，不加邊，啟用函式使用relu\n",
        "model.add(Conv2D(96, (11, 11), strides=(4, 4), input_shape=(227, 227, 3), padding='valid', activation='relu',kernel_initializer='uniform'))\n",
        "# 池化層\n",
        "model.add(MaxPooling2D(pool_size=(3, 3), strides=(2, 2)))\n",
        "# 第二層加邊使用256個5x5的卷積核，加邊，啟用函式為relu\n",
        "model.add(Conv2D(256, (5, 5), strides=(1, 1), padding='same', activation='relu', kernel_initializer='uniform'))\n",
        "#使用池化層，步長為2\n",
        "model.add(MaxPooling2D(pool_size=(3, 3), strides=(2, 2)))\n",
        "# 第三層卷積，大小為3x3的卷積核使用384個\n",
        "model.add(Conv2D(384, (3, 3), strides=(1, 1), padding='same', activation='relu', kernel_initializer='uniform'))\n",
        "# 第四層卷積,同第三層\n",
        "model.add(Conv2D(384, (3, 3), strides=(1, 1), padding='same', activation='relu', kernel_initializer='uniform'))\n",
        "# 第五層卷積使用的卷積核為256個，其他同上\n",
        "model.add(Conv2D(256, (3, 3), strides=(1, 1), padding='same', activation='relu', kernel_initializer='uniform'))\n",
        "model.add(MaxPooling2D(pool_size=(3, 3), strides=(2, 2)))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(4096, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(4096, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(NUM_CLASSES, activation='softmax'))\n",
        "model.compile(loss='categorical_crossentropy', optimizer='sgd', metrics=['accuracy']) #binary_crossentropy\n",
        "\n",
        "model.summary()\n",
        "\n"
      ],
      "metadata": {
        "id": "Hn7dqLU2REKI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 訓練模型\n",
        "History=model.fit_generator(train_batches,\n",
        "                        steps_per_epoch = train_batches.samples // BATCH_SIZE,\n",
        "                        validation_data = valid_batches,\n",
        "                        validation_steps = valid_batches.samples // BATCH_SIZE,\n",
        "                        epochs = NUM_EPOCHS,\n",
        "                        class_weight={0:0.850174216,1:1.213930348},\n",
        "                        callbacks = callbacks) \n",
        "\n",
        "#2 class: class_weight={0:0.850174216,1:1.213930348}\n",
        "#5 class: class_weight={0:0.346844,1:1.008696,2:2.109091,3:3.538983,4:2.711688}\n",
        "\n",
        "# 儲存訓練好的模型\n",
        "model.save(WEIGHTS_FINAL)"
      ],
      "metadata": {
        "id": "VpzvfwD_RIGZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}